%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Tabular RL}

\begin{document}

\twocolumn[
\icmltitle{
    Reinforcement Learning Assignment 1: \\
    Tabular Reinforcement Learning
}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Josef Hamelink}{LIACS}
\end{icmlauthorlist}

\icmlaffiliation{LIACS}{Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands}

\icmlcorrespondingauthor{Thomas Moerland}{LIACS}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Reinforcement Learning, RL}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This paper is a submission to the first assignment of the Reinforcement Learning course at Leiden University.
The basic concepts of reinforcement learning are introduced in a tabular setting.
Dynamic programming is also used to highlight its differences with reinforcement learning.
The problem used to illustrate the concepts in this assignment is a stochastic variation of the \textit{windy gridworld} problem \cite{SuttonBarto2018}.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Reinforcement learning (RL) is a subfield of machine learning that focuses on learning to act in an environment.
The actor is typically called an \textit{agent}.
The agent's goal is to maximize the expected cumulative reward it receives over time.
In this assignment, the agents try to achieve this goal by minimizing the number of steps it takes to reach the goal state.
The agents that have been modelled in this assignment are Q-learning (vanilla and n-step), SARSA, and Monte Carlo.
To benchmark their performance, a Dynamic Programming (DP) agent is also implemented, which is guaranteed to find the optimal policy.
All algorithms employ Q-tables, in which an estimate of the expected ``value'' of a state-action pair is stored for all possible state-action pairs.

\section{Problem description}
\label{sec:problem}

Because we are interested in the performance of various tabular model-free RL algorithms, a slightly modified version of the \textit{windy gridworld} problem \cite{SuttonBarto2018} suits our purposes.
The size of an agent's Q-table is $|\mathcal{S}| \times |\mathcal{A}|$, where $|\mathcal{S}|$ is the number of states and $|\mathcal{A}|$ is the number of actions the agent can take in each state.

In the case of this environment (Figure \ref{fig:env}), $|\mathcal{S}| = 7 \times 10 = 70$ and $|\mathcal{A}| = 4$, so the size of the Q-table is $70 \times 4 = 280$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\linewidth]{figs/env.png}
    \caption{The environment used in this assignment.}
    \label{fig:env}
\end{figure}

\newcommand{\fnZeroIndex}{\footnote{We begin indexing at 0, so column 3 is the fourth column to the left.}}

The agent starts at the square marked with an ``S'' and must reach the square marked with a ``G''.
The agent can move in all four directions, but the environment is stochastic.
In columns 3\fnZeroIndex, 4, 5, and 8, there is an 80\% chance that the agent will be blown upwards by one tile, meaning that when moving right from position $(2, 3)$, the agent will probably end up in position $(3, 4)$ instead of $(3, 3)$.
In columns 6 and 7, the chance is also 80\%, but the agent will be blown upwards by two tiles.
The agent receives a reward of $+40$ when it reaches the goal state, and a reward of $-1$ for any other tile it visits.

The optimal path -- assuming the wind always blows when it matters, i.e. the last two steps -- is shown in Figure \ref{fig:path}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\linewidth]{figs/path.png}
    \caption{The optimal path to the goal state.}
    \label{fig:path}
\end{figure}

This path gives a total reward of $- 1 \times 16 + 40 = +24$.
The average reward per timestep equates to $24 / 17 \approx 1.412$.

\section{Methods}
\label{sec:methods}

\subsection{Dynamic Programming}
\label{ssec:dp}

The definition of Dynamic Programming (DP) comes down to solving a complex problem by breaking it down into simpler subproblems.
In the case of RL, the problem is to find the optimal policy $\pi^*$, which is the policy that maximizes the expected cumulative reward.
In order to solve this, we can use the Bellman equation (\ref{eq:bellmanDP}).

\begin{equation}
    V^*(s) = \max_{a \in \mathcal{A}} \left( \sum_{s' \in \mathcal{S}} p(s', r | s, a) \left( r + \gamma V^*(s') \right) \right)
    \label{eq:bellmanDP}
\end{equation}

Here, $V^*(s)$ is the value of state $s$ under the optimal policy $\pi^*$, $\mathcal{A}$ is the set of actions, $\mathcal{S}$ is the set of states, $p(s', r | s, a)$ is the probability of transitioning from state $s$ to state $s'$ with reward $r$ when taking action $a$, $r$ is the reward, and $\gamma$ is the discount factor.
The way this equation leads to the optimal policy is by iteratively solving the equation for $V^*(s)$ for all $s \in \mathcal{S}$.
Concretely, we initialize $V^*(s)$ to $0$ for all $s \in \mathcal{S}$, and sweep over all states until the values converge.
The optimal policy is then given by the choosing the action that maximizes the value of the next state, as shown in Equation \ref{eq:policyDP}.

\begin{equation}
    \pi^*(s) = \arg \max_{a \in \mathcal{A}} \left( \sum_{s' \in \mathcal{S}} p(s', r | s, a) \left( r + \gamma V^*(s') \right) \right)
    \label{eq:policyDP}
\end{equation}

It is important to note that in order to take this approach, the transition probabilities must be known to the agent.
This is what differentiates DP from the Reinforcement Learning algorithms that will be discussed in the upcoming sections.

\subsection{Reinforcement Learning}
\label{ssec:rl}

In general, Reinforcement Learning (RL) is a framework for learning an optimal policy $\pi^*$ through direct interaction with the environment.
Experience is gathered by taking actions in the environment based on a behaviour policy $\pi_b$.
For all intents and purposes, $\pi_b$ can be just called $\pi$ in this section, as no distinction will be made between the behaviour policy and the target policy.
The policies that will be discussed in this section are the $\varepsilon$-greedy and Boltzmann (softmax) policies.

\subsubsection*{$\varepsilon$-greedy policy}
\label{sssec:egreedy}

The $\varepsilon$-greedy policy is a simple policy that chooses the action with the highest Q-value with probability $1 - \varepsilon$, and chooses a random (exploratory) action with probability $\varepsilon$.

\newpage

There are two main schools of thought on how to interpret $\varepsilon$.
One is that it is the probability of choosing a random action, and the other is that it is the probability of choosing a strictly exploratory action.
In this assignment, the latter interpretation will be used.
The policy is described in Equation \ref{eq:egreedy}.

\begin{equation}
    \pi(a | s) = \begin{cases}
        1 - \varepsilon & \text{if } a = \arg \max_{a' \in \mathcal{A}} Q(s, a') \\
        \frac{\varepsilon}{|\mathcal{A}|-1} & \text{otherwise}
    \end{cases}
\label{eq:egreedy}
\end{equation}

It is clear that a high value for $\varepsilon$ will lead to more exploration, while lower values will lead to a more greedy policy.

\subsubsection*{Boltzmann (softmax) policy}
\label{sssec:softmax}

The Boltzmann policy is a more sophisticated policy that assigns probabilities to each action based on the Q-values of the actions.
The probability of choosing an action is proportional to the Q-value of that action.
The policy is described in Equation \ref{eq:softmax}.

\begin{equation}
    \pi(a | s) = \frac{\exp \left( \frac{Q(s, a)}{\tau} \right)}{\sum_{a' \in \mathcal{A}} \exp \left( \frac{Q(s, a')}{\tau} \right)}
    \label{eq:softmax}
\end{equation}

The parameter $\tau$ is called the temperature, and it controls the amount of exploration.
High values of $\tau$ will lead to more exploration, as the probabilities will be more uniform.
As $\tau \to \infty$, $\pi(a | s)$ will approach $|\mathcal{A}|^{-1} \: \forall \: a \in \mathcal{A}$.
Conversely, as $\tau \to 0$, $\pi(a | s)$ will approach $1$ for the action with the highest Q-value, and $0$ for all other actions.

\subsubsection*{Q-learning}
\label{sssec:ql}

Q-learning is one of the most easy to implement and reason about RL algorithms.
Similarly to DP, Q-learning uses the highest value of the next state to update the value of the current state.
However, because it is model-free, it does not require the transition probabilities to be known to the agent.
Instead, it estimates a target value ($G_t$) for the current state-action pair, which is based on the optimistic estimate of the value of the next state-action pair.
It then updates the Q-value of the current state-action pair towards this target value.
This is shown in Equations \ref{eq:targetQ} and \ref{eq:updateQ}.

\begin{equation}
    G_t = r_t + \gamma \cdot \max_{a' \in \mathcal{A}} Q(s_{t+1}, a')
    \label{eq:targetQ}
\end{equation}

\begin{equation}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \cdot \left( G_t - Q(s_t, a_t) \right)
    \label{eq:updateQ}
\end{equation}

The implementation is shown in Algorithm \ref{alg:qlearning}.

\begin{algorithm}[htbp]
    \caption{Q-learning}
    \label{alg:qlearning}
 \begin{algorithmic}
    \STATE {\bfseries Input:} $budget, environment$
    \STATE initialize $Q(s, a)$ to $0$ for all $s \in \mathcal{S}$ and $a \in \mathcal{A}$
    \STATE $s \gets s_0$
    \REPEAT
    \STATE sample action $a$ using Equation \ref{eq:egreedy} or \ref{eq:softmax}
    \STATE take action $a$ in environment to get $r$ and $s'$
    \STATE update $Q(s, a)$ with Equations \ref{eq:targetQ} and \ref{eq:updateQ}
    \IF{$s'$ is terminal}
        \STATE $s \gets s_0$
    \ELSE
        \STATE $s \gets s'$
    \ENDIF
    \STATE $budget \gets budget - 1$
    \UNTIL{$budget = 0$}
\end{algorithmic}
\end{algorithm}

\subsubsection*{SARSA}
\label{sssec:sarsa}

SARSA is a model-free RL algorithm that is very similar to Q-learning.
The main difference is that it uses the action that is sampled from the behaviour policy to update the Q-value of the current state-action pair instead of using the action with the highest Q-value.
One way to frame this is that SARSA uses its own behaviour policy to update the Q-value of the current state-action pair, while Q-learning uses the greedy policy for this.
SARSA's update rule is identical to that of Q-learning (Equation \ref{eq:updateQ}), but the target value is calculated with Equation \ref{eq:targetS}:

\begin{equation}
    G_t = r_t + \gamma \cdot Q(s_{t+1}, a_{t+1})
    \label{eq:targetS}
\end{equation}

The implementation is shown in Algorithm \ref{alg:sarsa}.

\begin{algorithm}[htbp]
    \caption{SARSA}
    \label{alg:sarsa}
 \begin{algorithmic}
    \STATE {\bfseries Input:} $budget, environment$
    \STATE initialize $Q(s, a)$ to $0$ for all $s \in \mathcal{S}$ and $a \in \mathcal{A}$
    \STATE $s \gets s_0$
    \STATE sample action $a$ using Equation \ref{eq:egreedy} or \ref{eq:softmax}
    \REPEAT
    \STATE take action $a$ in environment to get $r$ and $s'$
    \STATE sample action $a'$ using Equation \ref{eq:egreedy} or \ref{eq:softmax}
    \STATE update $Q(s, a)$ with Equations \ref{eq:targetS} and \ref{eq:updateQ}
    \IF{$s'$ is terminal}
        \STATE $s \gets s_0$
        \STATE sample action $a$ using Equation \ref{eq:egreedy} or \ref{eq:softmax}
    \ELSE
        \STATE $s \gets s'$
        \STATE $a \gets a'$
    \ENDIF
    \STATE $budget \gets budget - 1$
    \UNTIL{$budget = 0$}
\end{algorithmic}
\end{algorithm}

\subsubsection*{$n$-step Q-learning}
\label{sssec:nstep}

$n$-step Q-learning is another temporal difference algorithm that is similar to Q-learning, but it uses the $n$-step return instead of the one-step return.
The $n$-step return is the sum of the (discounted) rewards over the next $n$ steps, plus a bootstrapped value of the $n$\textsuperscript{th} state-action pair.
The bootstrapped value is the Q-value of the $n$\textsuperscript{th} state-action pair, which is estimated using the greedy policy (hence the name Q-learning).
This means that $n$-step Q-learning is actually both on-policy and off-policy, assuming that the behaviour policy is not greedy.
The target value is calculated with either Equation \ref{eq:targetN1} or Equation \ref{eq:targetN2}, based on whether the episode ends before the $n$\textsuperscript{th} step or not.
If the episode ends before $n$ is reached, bootstrapping is not possible, so the second term can be omitted.

\begin{equation}
    G_t = \sum_{i=0}^{n-1} \left[ \gamma^i \cdot r_{t+i} \right] + \gamma^n \cdot \max_{a' \in \mathcal{A}} Q(s_{t+n}, a')
    \label{eq:targetN1}
\end{equation}

\begin{equation}
    G_t = \sum_{i=0}^{n-1} \left[ \gamma^i \cdot r_{t+i} \right]
    \label{eq:targetN2}
\end{equation}

Again, the same tabular update rule (Equation \ref{eq:updateQ}) is used to update the Q-value of the current state-action pair.
The implementation is shown in Algorithm \ref{alg:nstep}.
It should be noted that the lines between what constitutes a single step and what constitutes a single episode became very blurry at this point in the assignment.

\subsubsection*{Monte Carlo}
\label{sssec:montecarlo}

Monte Carlo methods are a class of algorithms that rely on the idea of ``full'' playouts.
This means that the agent plays out an entire episode, and then uses the rewards received during the episode to update the Q-values retroactively.
This quite similar to the $n$-step Q-learning algorithm, but instead of using the $n$-step return, the full return is used.
This full return can be thought of as the $n$-step return (Equation \ref{eq:targetN2}) with $n = \infty$.
The implementation of Monte Carlo for this assignment is shown in Algorithm \ref{alg:montecarlo}.

\newpage

\begin{algorithm}[htbp]
    \caption{$n$-step Q-learning}
    \label{alg:nstep}
 \begin{algorithmic}
    \STATE {\bfseries Input:} $budget, environment, n, max\_episode\_length$
    \STATE initialize $Q(s, a)$ to $0$ for all $s \in \mathcal{S}$ and $a \in \mathcal{A}$
    \REPEAT
    \STATE $s \gets s_0$
    \FOR{$t = 0 \dots max\_episode\_length$}
        \STATE sample action $a_t$ using Equation \ref{eq:egreedy} or \ref{eq:softmax}
        \STATE take action $a_t$ in environment to get $r_t$ and $s_{t+1}$
        \IF{$s_{t+1}$ is terminal}
            \STATE \textbf{break}
        \ENDIF
    \ENDFOR
    \STATE $episode\_length \gets t+1$
    \FOR{$t = 0 \dots \min(n, episode\_length)$}
        \STATE $m \gets \min(n, episode\_length - t)$
        \IF{$t + m < episode\_length$}
            \STATE update $Q(s_t, a_t)$ with Equation \ref{eq:targetN1} \\ (with bootstrapping)
        \ELSE
            \STATE update $Q(s_t, a_t)$ with Equation \ref{eq:targetN2} \\ (no bootstrapping)
        \ENDIF
    \ENDFOR
    \STATE $budget \gets budget - 1$
    \UNTIL{$budget = 0$}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
    \caption{Monte Carlo}
    \label{alg:montecarlo}
 \begin{algorithmic}
    \STATE {\bfseries Input:} $budget, environment, max\_episode\_length$
    \STATE initialize $Q(s, a)$ to $0$ for all $s \in \mathcal{S}$ and $a \in \mathcal{A}$
    \REPEAT
    \STATE $s \gets s_0$
    \FOR{$t = 0 \dots max\_episode\_length$}
        \STATE sample action $a_t$ using Equation \ref{eq:egreedy} or \ref{eq:softmax}
        \STATE take action $a_t$ in environment to get $r_t$ and $s_{t+1}$
        \IF{$s_{t+1}$ is terminal}
            \STATE \textbf{break}
        \ENDIF
    \ENDFOR
    \STATE $episode\_length \gets t+1$
    \FOR{$t = 0 \dots episode\_length$}
        \STATE update $Q(s_t, a_t)$ with Equation \ref{eq:targetN2} ($n = \infty$)
    \ENDFOR
    \STATE $budget \gets budget - 1$
    \UNTIL{$budget = 0$}
\end{algorithmic}
\end{algorithm}

\subsection{Experimental setup}
\label{sssec:setup}

The experiments were done with a small set of hyperparameters, because I believe the main focus of this assignment to be getting a feel for the different algorithms, rather than actually finding cool results.
In places where specific hyperparameters are omitted, the default values used are:

\begin{center}
    \begin{tabular}{|c|c|}
        \toprule
        parameter & value \\
        \midrule
        $\alpha$ & 0.25 \\
        $\gamma$ & 1.0 \\
        $\varepsilon$ & 0.1 \\
        $max\_episode\_length$ & 150 \\
        \bottomrule
    \end{tabular}
\end{center}

The following experiments have been run:

\begin{itemize}
    \item Q-learning with $\epsilon$-greedy vs softmax policies:
    \begin{itemize}
        \item $\epsilon$-greedy with $\epsilon \in \{0.02, 0.1, 0.3\}$.
        \item softmax with $\tau \in \{0.01, 0.1, 1\}$.
    \end{itemize}
    \item Q-learning vs. SARSA:
    \begin{itemize}
        \item $\alpha \in \{0.02, 0.1, 0.4\}$.
    \end{itemize}
    \item $n$-step Q-learning with $n \in \{1, 3, 10, 30\}$ vs. Monte Carlo.
\end{itemize}

The rewards are always averaged over $50$ independent runs, and the learning curves are smoothed to make the figures more interpretable.
For all RL agents, a greedy evaluation run was done after each $500$ timesteps to get a more complete picture of the performance.


\section{Results}
\label{sec:results}

\subsection{Dynamic Programming}
\label{ssec:resdp}

In order to provide a baseline for the RL algorithms, we compute the optimal policy using dynamic programming with Q-value iteration.
The optimal average reward as described in Section \ref{sec:problem} is successfully reached following convergence of the algorithm.
This can be seen in Figure \ref{fig:dp17}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\linewidth]{figs/dp17.png}
    \caption{Policy after $17$ iterations of dynamic programming.}
    \label{fig:dp17}
\end{figure}

We see that all arrows in Figure \ref{fig:path} (the optimal path) are also present in Figure \ref{fig:dp17}, which is a good sign that the dynamic programming algorithm has found the optimal policy.
Of course, there is a probability that no wind blows for the first five consecutive steps, which would make the optimal result in an average reward of $(6 \times -1 + 40) / 7 \approx 5.14$.
The chance of this occurring however is $0.2^5 = 3.2 \cdot 10^{-4}$, which is negligible.
We can appreciate that the dynamic programming algorithm is able to find the optimal policy accounting for all possible wind interactions.

If we take a look at the process (Figures \ref{fig:dp0} \& \ref{fig:dp7}), we can see that the first values to be updated with any significance are the ones that are close to the goal state, and this information propagates backwards towards the initial state.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\linewidth]{figs/DP0.png}
    \caption{Dynamic Programming after the $0$\textsuperscript{th} iteration.}
    \label{fig:dp0}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\linewidth]{figs/DP7.png}
    \caption{Dynamic Programming after the $7$\textsuperscript{th} iteration.}
    \label{fig:dp7}
\end{figure}

Computing $V^*(s_0)$ can be done by summing the rewards of all possible paths from $s_0$ to the goal state, weighted by the probability of each path.
Doing this manually is tedious, so I printed $V^*(s)$ for all $s \in \mathrm{optimal\ path}$, and $V^*(s_0)$ turned out to be $\pm 23.312$.
Something that stands out is that for all $(s, a, s')$ pairs where the transition probability is $1$, $\Delta(V^*(s'), V^*(s)) = -1$, i.e. the reward for landing in that non-terminal state.

Changing the location of the goal state to $(6, 2)$ results in a very different policy, where the agent does not cross the windy columns, but instead tries to stick to the lower side of the grid.
The optimal average reward is then even lower, but the path is less stable, as the wind has more of an impact on how fast the agent manages to reach the goal.

\subsection{Exploration}
\label{ssec:resexpl}

The first experiment was to compare the performance of $\epsilon$-greedy and softmax policies.
For simplicity's sake, only Q-learning was used.
The results are shown in Figure \ref{fig:exploration}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/exploration.png}
    \caption{Performance of $\varepsilon$-greedy and softmax policies with Q-learning.}
    \label{fig:exploration}
\end{figure}

When looking at the learning curves, two ``losers'' are immediately apparent: $\varepsilon = 0.3$ and $\tau = 1.0$; the exploratory settings.
The agent with $\varepsilon = 0.3$ is forced to take random actions too often, even when it has already learned a decent policy.
The softmax agent with $\tau = 1.0$ is not nearly as bad, but it does take a little longer to converge than the other four agents, which seem to be very similar.

If we shift our focus to the greedy evaluation curves however, a very different picture emerges.
The $\varepsilon = 0.3$ agent is actually the best performing agent, and the softmax agent with $\tau = 0.01$ is the worst.
This makes sense, because the $\varepsilon = 0.3$ agent is forced to explore more, and thus has a better idea of what actions are worthwhile, even when the environment blows it off its course.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/QL.png}
    \caption{Policy learned by Q-learning with $\varepsilon, \alpha = 0.1$.}
    \label{fig:policyQ}
\end{figure}

As can be seen in Figure \ref{fig:policyQ}, the resulting policy is quite similar to the DP one, but it is a bit more noisy.

\subsection{On-Policy vs. Off-Policy}
\label{ssec:resonoff}

The next experiment was to compare the performance of Q-learning and SARSA.
The results are shown in Figure \ref{fig:onoff}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/on_off_policy.png}
    \caption{Performance of Q-learning and SARSA.}
    \label{fig:onoff}
\end{figure}

In general, Q-learning seems to be performing better than SARSA with the same hyperparameters, but the difference is not significant when looking at the learning curves alone.
This is to be expected, because the two algorithms are very similar, and the only difference is that SARSA uses its own policy to choose the next action in the update rule, whereas Q-learning uses the greedy policy.
The most notable observation to be made is that a learning rate of $0.02$ is not reasonable for either algorithm, as both their learning curves as well as their greedy evaluation curves are extremely slow to converge.
Personally, Q-learning with $\alpha = 0.4$ is my favorite; it leaves its SARSA counterpart in the dust when it comes to the greedy evaluation curves.

\subsection{$\mathbf{n}$-step and Monte Carlo}
\label{ssec:resnm}

In the final experiment we compare the performance of $n$-step and Monte Carlo methods.
The results are shown in Figure \ref{fig:nm}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/depth.png}
    \caption{Performance of $n$-step and Monte Carlo methods.}
    \label{fig:nm}
\end{figure}

The $n$-step methods seem to be performing better than the Monte Carlo methods.
This is because sadly, $n$-step methods are not viable for this problem, and Monte Carlo is basically $n$-step with $n = \infty$.
My intuition is that this is due to the fact that these methods are not robust against stochasticity, and the wind in this problem is quite noisy.
There is a very clear trend in both the learning curves and the greedy evaluation curves, where the $n$-step methods are performing better the lower the value of $n$, with $n = 1$ being the best performing method.

\newpage

In order to illustrate how embarrassing the performance of these $n$-step methods is this problem, the resulting policy after a very generous budget (algorithm has converged) is shown in Figure \ref{fig:policyn}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/MC.png}
    \caption{Policy learned by Monte Carlo.}
    \label{fig:policyn}
\end{figure}

\newpage

\section{Conclusion and discussion}
\label{sec:concdisc}

In this report, we have discussed the implementation of several reinforcement learning algorithms and compared their performance on the windy gridworld problem.
We have also shown that Dynamic Programming is the best choice of algorithm for this problem.
Of course, Reinforcement Learning should not be discounted, as it is a very powerful tool for learning from experience, and it can be used to solve problems that are not solvable by Dynamic Programming, e.g. problems where no model of the environment is available.
It should also be noted that both DP and RL suffer from the curse of dimensionality, but RL has a trick up its sleeve; it can employ neural networks to approximate the value function, which is a very powerful tool for solving problems with a very large (or even continuous) state space.

When deciding between $\varepsilon$-greedy and softmax policies, I personally lean more towards $\varepsilon$-greedy for this problem, as it is easier to reason about, and it is also (ever-so-slightly) more computationally efficient.

It was disappointing to see that the $n$-step methods performed so poorly, but I think that this is due to the fact that the problem is not well suited for these methods, and that they are not as robust against stochasticity as their single step counterparts.
I do however think that it was a good idea to include them in the report, as it shows that it is not always a good idea to simply throw more compute at a problem.

\bibliography{main}
\bibliographystyle{icml2021}

\end{document}
